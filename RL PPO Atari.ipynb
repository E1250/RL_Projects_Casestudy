{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Youtube video - https://www.youtube.com/watch?v=xHf8oKd7cgU&list=PLgMYKvjKE10UZNku-Qx7-z2PEC-7KLiUn&index=2\n",
    "* Proximal Policy Optimization Algorithms - https://arxiv.org/abs/1707.06347"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import ale_py\n",
    "gym.register_envs(ale_py)\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3.common.atari_wrappers import MaxAndSkipEnv\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, nb_actions):\n",
    "        super().__init__()\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Conv2d(4, 16, kernel_size=8, stride=4),\n",
    "            nn.Tanh(),\n",
    "            nn.Conv2d(16, 32, kernel_size=4, stride=2),\n",
    "            nn.Tanh(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32*9*9, 256),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(256, nb_actions)\n",
    "        )\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.head(x)\n",
    "        return self.actor(h), self.critic(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environments():\n",
    "    def __init__(self, nb_actor):\n",
    "        self.envs = [self.get_env() for _ in range(nb_actor)]\n",
    "        self.observations = [None for _ in range(nb_actor)]\n",
    "        self.current_life = [None for _ in range(nb_actor)]\n",
    "        self.done = [False for _ in range(nb_actor)]\n",
    "        self.total_reward = [0 for _ in range(nb_actor)]\n",
    "        self.nb_actor = nb_actor\n",
    "\n",
    "        for env_id in range(nb_actor):\n",
    "            self.reset_env(env_id)\n",
    "\n",
    "    def len(self):\n",
    "        return self.nb_actor\n",
    "    \n",
    "    def reset_env(self, env_id):\n",
    "        self.total_reward[env_id] = 0\n",
    "        self.envs[env_id].reset()   \n",
    "\n",
    "        for _ in range(random.randint(1, 30)):\n",
    "            self.observations[env_id], reward, _, _, info = self.envs[env_id].step(1)\n",
    "            self.total_reward[env_id] += reward\n",
    "            self.current_life[env_id] = info['lives']\n",
    "\n",
    "    def step(self, env_id, action):\n",
    "        next_obs, reward, dead, _, info = self.envs[env_id].step(action)\n",
    "        done = True if (info['lives'] < self.current_life[env_id]) else False\n",
    "        self.done = done\n",
    "        self.total_reward[env_id] += reward\n",
    "        self.current_life[env_id] = info['lives']\n",
    "        self.observations[env_id] = next_obs\n",
    "        return next_obs, reward, dead, done, info\n",
    "    \n",
    "    def get_env(self):\n",
    "        env = gym.make(\"ALE/Breakout-v5\", \n",
    "               render_mode='human'\n",
    "               )\n",
    "        env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "        env = gym.wrappers.ResizeObservation(env, (84, 84))\n",
    "        env = gym.wrappers.GrayscaleObservation(env)\n",
    "        env = gym.wrappers.FrameStackObservation(env, 4) # It was frame stack in video\n",
    "        env = MaxAndSkipEnv(env, skip = 4)\n",
    "        return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PPO(\n",
    "        envs,\n",
    "        T=128,\n",
    "        K=3,\n",
    "        batch_size=32*8,\n",
    "        gamma=0.99,\n",
    "        device=\"cuda\",\n",
    "        gae_parameter=0.95,\n",
    "        vf_coef_c1=1,\n",
    "        ent_coef_c2=0.01,\n",
    "        nb_iterations=40_000\n",
    "    ):\n",
    "    optimizer = torch.optim.Adam(actorcritic.parameters(), lr=2.5e-4)\n",
    "    sheduler = torch.optim.lr_scheduler.LinearLR(\n",
    "        optimizer, \n",
    "        start_factor=1,\n",
    "        end_factor=0,\n",
    "        total_iters=nb_iterations\n",
    "    )\n",
    "    max_reward = 0\n",
    "    total_rewards = [[] for _ in range(envs.len())]\n",
    "    smoothed_rewareds = [[] for _ in range(envs.len())]\n",
    "\n",
    "    for iteration in tqdm(range(nb_iterations)):\n",
    "        advantages = torch.zeros((envs.len(), T), dtype=torch.float32).to(device)\n",
    "        buffer_states = torch.zeros((envs.len(), T, 4, 84, 84), dtype=torch.float32).to(device)\n",
    "        buffer_actions = torch.zeros((envs.len(), T), dtype=torch.long).to(device)\n",
    "        buffer_logprobs = torch.zeros((envs.len(), T), dtype=torch.float32).to(device)\n",
    "        buffer_states_values = torch.zeros((envs.len(), T+1), dtype=torch.float32).to(device)\n",
    "        buffer_rewards = torch.zeros((envs.len(), T), dtype=torch.float32).to(device)\n",
    "        buffer_in_terminal = torch.zeros((envs.len(), T), dtype=torch.float32).to(device)\n",
    "\n",
    "        for env_id in range(envs.len()):\n",
    "            with torch.no_grad():\n",
    "                for t in range(T):\n",
    "                    obs = torch.tensor(envs.observations[env_id] / 255, dtype=torch.float32).to(device).unsqueeze(0)\n",
    "                    logits, value = actorcritic(obs)\n",
    "                    logits, value = logits.squeeze(0), value.squeeze(0)\n",
    "                    m = torch.distributions.Categorical(logits=logits)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "e1250",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
