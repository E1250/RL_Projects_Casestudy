{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Human-level control through deep reinforcement learning - https://www.nature.com/articles/nature14236\n",
    "* Youtube video - https://www.youtube.com/watch?v=atqneVERSMg&list=PLgMYKvjKE10UZNku-Qx7-z2PEC-7KLiUn&index=9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import random\n",
    "import gymnasium as gym\n",
    "import ale_py\n",
    "gym.register_envs(ale_py)\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3.common.buffers import ReplayBuffer\n",
    "from stable_baselines3.common.atari_wrappers import MaxAndSkipEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, nb_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Conv2d(4, 16, 8, stride=4), \n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, 4, stride=4), \n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(), \n",
    "            nn.Linear(2592, 256), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, nb_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x / 255.0)  # Normalize input as Uint8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Deep_Q_Learning(\n",
    "        env, \n",
    "        batch_size = 32,\n",
    "        M = 30_000_000,  #  48 to 72 hours of training on a modern GPU to converge (30_000_000)\n",
    "        epsilon_start = 1.0,  # Random Action\n",
    "        epsilon_end = 0.01,  # Best Action\n",
    "        nb_exploration_steps = 1_000_000, \n",
    "        buffer_size=1_000_000,  # This one is important.\n",
    "        gamma = 0.99,\n",
    "        update_frequency = 4,\n",
    "        training_start_it = 80_000,  # Don't train directly, let agent explore first with a random policy\n",
    "        C = 10_000,\n",
    "        device = 'cuda'\n",
    "        ):\n",
    "    \n",
    "    # Initialize replay memory D to capacity N\n",
    "    rb = ReplayBuffer(buffer_size,\n",
    "                      env.observation_space,\n",
    "                      env.action_space,\n",
    "                      device=device,\n",
    "                      n_envs=1,\n",
    "                      optimize_memory_usage = True,\n",
    "                      handle_timeout_termination = False)\n",
    "    \n",
    "    # Initialize action-value function Q with random weights\n",
    "    q_network = DQN(env.action_space.n).to(device)\n",
    "    # Initialize target action-value function Q_hat\n",
    "    target_q_network = DQN(env.action_space.n).to(device)\n",
    "    target_q_network.load_state_dict(q_network.state_dict())\n",
    "\n",
    "    optimizer = torch.optim.Adam(q_network.parameters(), lr=1.25e-4)\n",
    "\n",
    "     # To plot later\n",
    "    smoothed_rewards = []\n",
    "    rewards = []\n",
    "    max_reward = 0\n",
    "\n",
    "    epoch = 0\n",
    "\n",
    "    progress_bar = tqdm(total=M)\n",
    "    while epoch <= M:\n",
    "\n",
    "        # Initialize sequense s1 = {x1} and preprocessed sequence φ1 = φ(s1)\n",
    "        state = env.reset()\n",
    "        dead = False\n",
    "        total_rewards = 0\n",
    "\n",
    "        for _ in range(random.randint(1, 30)):  # Noop and fire to reset the environment\n",
    "            obs, _, _, _, info = env.step(1)\n",
    "\n",
    "        # For t=1, T do\n",
    "        while not dead:\n",
    "            epsilon = max((epsilon_end - epsilon_start) * epoch / nb_exploration_steps + epsilon_start, epsilon_end)\n",
    "\n",
    "            if random.random() < epsilon: # With probability ε select a random action\n",
    "                action = np.array(env.action_space.sample())\n",
    "            else: # Otherwise select the best action a = max_a Q(φ(s), a; θ)\n",
    "                with torch.no_grad():\n",
    "                    q = q_network(torch.tensor(state).unsqueeze(0).to(device))\n",
    "                    action = torch.argmax(q, dim = 1).item()\n",
    "\n",
    "            # Execute action a in emulator and observe reward r and image x2\n",
    "            current_life = info['lives']\n",
    "            obs, reward, dead, _, info = env.step(action)\n",
    "\n",
    "            done = info['lives'] < current_life\n",
    "\n",
    "            total_rewards += reward\n",
    "            reward = np.sign(reward)  # Clip rewards to -1, 0, 1\n",
    "\n",
    "            # set st+1 = st, at, xt+1 and preprocess φt+1 = φ(st+1)\n",
    "            next_state = obs.copy()\n",
    "\n",
    "            # Store transition (φt, at, rt, φt+1) in D\n",
    "            rb.add(state[0], next_state, action, reward, done, info) # Store transition in the replay buffer\n",
    "\n",
    "            if epoch > training_start_it and epoch % update_frequency == 0:\n",
    "                # Sample random minibatch of transitions (φj, aj, rj, φj+1) from D\n",
    "                batch = rb.sample(batch_size)\n",
    "                with torch.no_grad():\n",
    "                    max_q_value_next_state = target_q_network(batch.next_observations).max(dim=1).values\n",
    "                    y_j = batch.rewards.squeeze(-1) + gamma * max_q_value_next_state * (1 - batch.dones.squeeze(-1).float())\n",
    "                \n",
    "                current_q_values = q_network(batch.observations).gather(1, batch.actions).squeeze(-1)\n",
    "                \n",
    "                loss = torch.nn.functional.huber_loss(y_j, current_q_values)\n",
    "\n",
    "                # Perform a gradient descent step according to equation 3\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            if (epoch % 50_000 == 0) and epoch > 0:\n",
    "                smoothed_rewards.append(np.mean(rewards))\n",
    "                rewards = []\n",
    "                plt.plot(smoothed_rewards)\n",
    "                plt.title(\"Average Reward on Breakout\")\n",
    "                plt.xlabel(\"Training Epochs\")\n",
    "                plt.ylabel(\"Average Reward per Episode\")\n",
    "                plt.show()\n",
    "            \n",
    "            epoch += 1\n",
    "            if epoch % C == 0:\n",
    "                target_q_network.load_state_dict(q_network.state_dict())\n",
    "\n",
    "            state = obs\n",
    "            progress_bar.update(1)\n",
    "\n",
    "        rewards.append(total_rewards)  \n",
    "\n",
    "        if total_rewards > max_reward:\n",
    "            max_reward = total_rewards\n",
    "            torch.save(q_network.cpu(), 'target_q_network_{epoch}_{max_reward}.pth')\n",
    "            q_network.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "env = gym.make(\"ALE/Breakout-v5\", \n",
    "               render_mode='human'\n",
    "               )\n",
    "env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "env = gym.wrappers.ResizeObservation(env, (84, 84))\n",
    "env = gym.wrappers.GrayscaleObservation(env)\n",
    "env = gym.wrappers.FrameStackObservation(env, 4) # It was frame stack in video\n",
    "env = MaxAndSkipEnv(env, skip = 4)\n",
    "\n",
    "Deep_Q_Learning(env, device = 'cuda',\n",
    "                M = 1000,\n",
    "                buffer_size = 100_000)\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "e1250",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
